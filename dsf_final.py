# -*- coding: utf-8 -*-
"""dsf_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZI24dBhGgAyUn8gi-8166R1XaiHDFx1F

The dataset that I will use for this asssingment contains information taken from UCI heart disease patients from 4 different hospitals these are:
1. Hungarian Institute of Cardiology,Budapest. 
2. University Hospital, Zurich, Switzerland. 
3. University Hospital, Basel, Switzerland and 
4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation.

The columns of the dataset contain the following variables:
1. age
2. sex
3. chest pain type (4 values)
4. resting blood pressure
5. serum cholestoral in mg/dl
6. fasting blood sugar > 120 mg/dl
7. resting electrocardiographic results (values 0,1,2)
8. maximum heart rate achieved
9. exercise induced angina
10. oldpeak = ST depression induced by exercise relative to rest
11. the slope of the peak exercise ST segment
12. number of major vessels (0-3) colored by flourosopy
13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect
14. Heart disease 1, no heart disease 0


During this project I will using all the features available try and build a model that accurately predicts if a patient does suffer or not from accute heart disease. 

First however we will clean the data and do a brief exploratory analysis of it.
"""

!pip install sklearn

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
import numpy as np
import imageio as imageio
from imageio import imread
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import cv2
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style

uci = pd.read_csv("heart.csv")
uci.head()

uci.shape

uci2 = uci.isnull().value_counts()
uci2
#There are no null values

uci.describe()

#We now use a bar plot to visualize how split our target is.

sns.barplot(uci.target.value_counts().index,100*uci.target.value_counts(normalize = True))
plt.title('Target distribution')
plt.xlabel('Target')
plt.ylabel('percentage %')
plt.show()

#As we can see our target value is reasonably split.

#If we want to further explore the data we could also plot a bar chart to see how accute heart disease affects different genders.
plt.figure(figsize=(20,5))
plt.subplot(1,2,1)
sns.barplot(uci[uci.sex == 1].target.value_counts().index,uci[uci.sex == 1].target.value_counts(normalize = True))
plt.xlabel('Target')
plt.ylabel('Percentage %')
plt.title('Male vs Target')
plt.subplot(1,2,2)
sns.barplot(uci[uci.sex == 0].target.value_counts().index,uci[uci.sex == 0].target.value_counts(normalize = True))
plt.xlabel('Target')
plt.ylabel('Percentage %')
plt.title('Female vs Target')
plt.show()

#As we can see women are more prone to develope heart disease than men.

#We can also plot the distribution of some key variables.

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(uci.trestbps)
plt.title('Resting blood pressure distribution')
#As we can see resting blood pressure is unevenly distributed with a slight skew to the left

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(uci.thalach)
plt.title('maximum heart rate achieved distribution')
#Again we can see maximum heart rate achieved seems slightly uneverly distributed with a skew to the right

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(uci.chol)
plt.title('cholesterol distribution')
#Cholesterol seems more evenly distributed with a mean probably around 210 and a skew to the left

plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
sns.distplot(uci.age)
plt.title('age')
#Age shows a clearly uneven distribution.



uci_corr = uci.corr()
uci_corr > 0.5
#Thanks to pandas corr tool we can also see that no values are strongly correlated.

uci_corr[uci_corr == True].count()

uci3 = uci.corr()
uci3
#We show here the actual values

sns.pairplot(uci)
#Finally we display some pairplots to show some potentially interesting relationships between features.

"""Now we finally start to build the model that will predict if a patient has accute heart disease"""

X = uci.drop('target',1)
y = uci.target
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.5,random_state=0)
#We split our data into train test split into a 50%/50% split in order to perform a cross validation later on

#Given the wide range of values we proceed to normalize the data in order to be able to feed it to an algorithm
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)
X_train = pd.DataFrame(X_train, columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal'])
X_train.head()
X_test = pd.DataFrame(X_test, columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach','exang', 'oldpeak', 'slope', 'ca', 'thal'])
X_test.head()

"""Now we need to choose an algorithm. In order to do this we will use sklearns diagram that allows us to choose the one that best suites our data. https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
 We understand that we have continous data from which we are trying to predict a certain category (1 ill or 0 healthy) the diagram then indicates us that since we have labeled data and less than 100k observations we should use a linear svc to predict our target
"""

from sklearn.svm import LinearSVC
from sklearn import metrics
from sklearn.metrics import accuracy_score
model = LinearSVC()
y2_model = model.fit(X_train, y_train)
y1_model = model.fit(X_test, y_test)
y2_predict =  y2_model.predict(X_test)
y1_predict = y1_model.predict(X_train)
print("accuracy set's 1 and 2:", accuracy_score(y_train, y1_predict), accuracy_score(y_test, y2_predict))
print("recall set's 1 and 2:", metrics.recall_score(y_train, y1_predict),metrics.recall_score(y_test, y2_predict))
print("precision score set's 1 and 2:",metrics.precision_score(y_train,y1_predict),metrics.precision_score(y_test,y2_predict))

#Appyling both models we get decent accuracies for both validation sets (both above 80%) with good recall scores and moderately good precision scores.

import pickle
filename = 'finalized_model.sav'
pickle.dump(y2_model, open(filename, 'wb'))

